{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # **Implementing Logistic regression from scratch without Scikit Learn**"
      ],
      "metadata": {
        "id": "nL5mmuxy71vr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing the Libraries"
      ],
      "metadata": {
        "id": "1kea4a5eSHOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import datasets"
      ],
      "metadata": {
        "id": "SAQHOF-NafwT"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Defining Sigmoid function"
      ],
      "metadata": {
        "id": "FzawP97dSMY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sigmoid Function to calculate the probabillity from the linear function that Z  = w.T + b i.e weights transpose +  bias\n",
        "def sigmoid(z):\n",
        "  return 1/(1+np.exp(-z))"
      ],
      "metadata": {
        "id": "j0wZvxHbbbzk"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a class of Logistic Regression and pass the paramaters line number of iterations , Learing Rate, Weights and Bias.\n",
        "- Then Two methods will be created called fit() method and predict method().\n",
        "- Inside the fit method , optimization function called gradient descent will be implemented which will helps us to find the Optimum weights and bias values which will be used to make the prediction."
      ],
      "metadata": {
        "id": "hriLVuuzGL9z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression Class"
      ],
      "metadata": {
        "id": "qHR8IKcLSTH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LogisticRegression:\n",
        "\n",
        "  def __init__(self,lr=0.001,n_iterations=1000):\n",
        "    self.lr = lr\n",
        "    self.n_iterations = n_iterations\n",
        "    self.weights = None\n",
        "    self.bias = None\n",
        "\n",
        "# fit method to train the model\n",
        "  def fit(self,X,y):\n",
        "    n_samples,n_features = X.shape   # Total number of features and total number of samples\n",
        "    self.weights = np.zeros(n_features)  # Intialising the weights with Zero which is equal to number of features because equation is represented as \n",
        "                                          # b0 + b1x1 + b2x2 + ........ + bnXn   so, each feature is getting multiplied with weights so, its should be equal\n",
        "                                          # to number of features.\n",
        "    self.bias = 0  # Bias will  be a constant so just intializing it with Zero\n",
        "\n",
        "    # This is for optimizing the the gradient descent function\n",
        "    for _ in range(self.n_iterations):\n",
        "      linear_prediction = np.dot(X,self.weights) + self.bias  # Getting the value of this function b0 + b1x1 + b2x2 + ........ + bnXn \n",
        "                                                  # written as X into weights here not doing  transpose of anything because let say X represent matrix of all\n",
        "                                                  # samples so X is let say m X n matrix where m is the number of samples and n is the number of features\n",
        "                                                  # since weights W is the same as n i.e. number of features and when we take matrix multiplication of [m,n] for X\n",
        "                                                  # and [n,1] for weights then no transpose is required because no. of columns of X == number of rows of W\n",
        "      \n",
        "      prediction = sigmoid(linear_prediction)  # Once we get the linear function we pass it through the sigmoid function which bounds the range of linear_prediction\n",
        "                                                # from - inf to inf between [0,1] i.e it shows the probability of samples\n",
        "\n",
        "      # Cost function for logistic regression is  : -[y log(yhat) + (1-y) log(1-yhat)]\n",
        "      # y : actual values\n",
        "      # yhat : prediction , i.e.probability after applying sigmoid function , yhat= sigmoid(Z) = sigmoid(W.T * X + b)\n",
        "\n",
        "      # Now this cost function should be differentiated to wrt to dw and db , so that cost could be minimised\n",
        "      # after differentiating by applying chain rule\n",
        "\n",
        "      # dw = X[A-Y]\n",
        "      # db = [A-Y]\n",
        "      # Where X : Matrix of all the samples , A : Prediction ,Y: Actual values\n",
        "\n",
        "      dw = 1/(n_samples) * np.dot(X.T,(prediction - y))  # Here we have done the trasnpose because X is [m,n] and (prediction-y) is [m,1] as prediction is \n",
        "                                                          # is calculated for all the samples passed , so matrix multiplication will not take place and so\n",
        "                                                          # transpose of X is taken then it will become [n,m]\n",
        "      db = 1/(n_samples) * np.sum(prediction-y)\n",
        "\n",
        "      \n",
        "      # Once the derivatives are calculated the we will updated the value of weights and bias\n",
        "      self.weights  = self.weights - (self.lr * dw)\n",
        "      self.bias = self.bias - (self.lr * db)\n",
        "\n",
        "# This is the prediction function that is passed here.\n",
        "  def predict(self,X):\n",
        "      linear_prediction = np.dot(X,self.weights) + self.bias\n",
        "      prediction = sigmoid(linear_prediction)\n",
        "\n",
        "    # After getting the precition we check if it is less than 0.5 then 0 else 1\n",
        "      yhat = [0 if i<=0.5 else 1 for i in prediction]\n",
        "      return yhat\n"
      ],
      "metadata": {
        "id": "aWaOE3TB7_s3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Accuracy"
      ],
      "metadata": {
        "id": "1evkeh4ZSbi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To Check the accuracy of the model\n",
        "# accuracy = TP + TN/(TP + FP + TN + FN)\n",
        "def accuracy(y_pred,y_test):\n",
        "  accuracy_rate = round((np.sum(y_pred==y_test)/len(y_test))*100,4) # Rounded the values around 4\n",
        "  print(accuracy_rate)"
      ],
      "metadata": {
        "id": "MKTnC_4elLDv"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data from Scikit Learn "
      ],
      "metadata": {
        "id": "QuSuxPQSSiw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bc = datasets.load_breast_cancer()"
      ],
      "metadata": {
        "id": "JYnoJfz3g4Vq"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = bc.data,bc.target"
      ],
      "metadata": {
        "id": "hm5ZDbfpigZc"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 30 numeric features\n",
        "X[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Jv-Cvrtilrn",
        "outputId": "20bd315b-91df-4397-95e1-92a9ed1b10a8"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.799e+01, 1.038e+01, 1.228e+02, 1.001e+03, 1.184e-01, 2.776e-01,\n",
              "       3.001e-01, 1.471e-01, 2.419e-01, 7.871e-02, 1.095e+00, 9.053e-01,\n",
              "       8.589e+00, 1.534e+02, 6.399e-03, 4.904e-02, 5.373e-02, 1.587e-02,\n",
              "       3.003e-02, 6.193e-03, 2.538e+01, 1.733e+01, 1.846e+02, 2.019e+03,\n",
              "       1.622e-01, 6.656e-01, 7.119e-01, 2.654e-01, 4.601e-01, 1.189e-01])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape # There are 569 samples and 30 features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0xAJJvqimsH",
        "outputId": "b89b3913-c628-4598-d09c-8911e61752f3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569, 30)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bwSAH6ri5Nk",
        "outputId": "4f42c9fa-b9e4-40c9-8e03-6cc08e564f78"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(569,)"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting the training and testing data\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state =23)"
      ],
      "metadata": {
        "id": "TpLldXyEjJ0-"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the Logistic Regression class object"
      ],
      "metadata": {
        "id": "VKWYTTPpS4py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression() # Object created without passing any values\n",
        "clf.fit(X_train,y_train)   # fit method is called\n",
        "ypred = clf.predict(X_test)  # predict method is called and it's return values is collected in the ypred function\n",
        "accuracy(ypred,y_test)   # then accuracy function is called to print the accuracy"
      ],
      "metadata": {
        "id": "ArOjjVilg4ar",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9159dcdd-8ed0-4140-9eb3-679752753382"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "94.7368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4ca8e5309727>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-z))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the object by changing the parameters"
      ],
      "metadata": {
        "id": "6OORDaWtUUyx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf2 = LogisticRegression(lr = 0.01,n_iterations=100) # Obejct created with some parametes\n",
        "clf2.fit(X_train,y_train)\n",
        "ypred = clf2.predict(X_test)\n",
        "accuracy(ypred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Unt9WFm4IK",
        "outputId": "8d82abd9-f1f2-4227-aaa5-1777cf9709e0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "76.3158\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4ca8e5309727>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-z))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf3 = LogisticRegression(lr = 0.001,n_iterations=100)\n",
        "clf3.fit(X_train,y_train)\n",
        "ypred = clf3.predict(X_test)\n",
        "accuracy(ypred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7WHk-fNnSt1",
        "outputId": "85959334-ab84-44d8-f5ae-0b39e643d9db"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "77.193\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4ca8e5309727>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-z))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clf4 = LogisticRegression(lr = 0.001,n_iterations=100000)\n",
        "clf4.fit(X_train,y_train)\n",
        "ypred = clf4.predict(X_test)\n",
        "accuracy(ypred,y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FB1t9avCnXql",
        "outputId": "8d78c111-c290-4339-8fb5-26e4671103e5"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-4ca8e5309727>:3: RuntimeWarning: overflow encountered in exp\n",
            "  return 1/(1+np.exp(-z))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.3509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "a4LRUm9Bnl-E"
      },
      "execution_count": 18,
      "outputs": []
    }
  ]
}